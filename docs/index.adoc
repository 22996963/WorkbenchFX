= Bachelor Thesis: WorkbenchFX
François Martin; Marco Sanfratello

FHNW University of Applied Sciences, Windisch

17^th^ of August 2018

Customer: http://www.dlsc.com[Dirk Lemmermann Software & Consulting] +
Advisor: mailto:dieter.holz@fhnw.ch[Dieter Holz‚ FHNW University of Applied Sciences] +
Examiner: https://www.inventage.com/[Edwin Steiner‚ Inventage]

// Check symbol
:y: &#10003;
// Ensures references are displayed in "Figure x" format
:xrefstyle: short

== Abstract

== Acknowledgements

== Introduction
// TODO: (MARCO)
// TODO: (=>Kopie von Readme mit Open source)
The paradigm of WorkbenchFX is: "Make everything possible. Expect the least."

// TODO: differentiate somewhere the terms "implementor", "user" and "API user"

== Motivation
// TODO: (MARCO)
// TODO: Ausgangslage / Vision

== Analysis

=== Persona
Who are the persona?

[cols="1,1,1"]
.Persona
|===
|link:include/persona/stefanie_berner.pdf[image:include/persona/stefanie_berner.png[Stefanie Berner]]
|link:include/persona/fabian_zimmer.pdf[image:include/persona/fabian_zimmer.png[Fabian Zimmer]]
|link:include/persona/anna_leutner.pdf[image:include/persona/anna_leutner.png[Anna Leutner]]
|===

=== Applications
// TODO: add text here

==== Blender (Tool for creating 3D models)
// TODO: (MARCO)
* Blender is divided up into five sections.
** Header --> Most important and the common settings
** Left Bar --> Tools
** Right Bar --> Tools
** Footer --> Animation, and view-modes
** Center --> The model, which is created
* All sections are resizable
* If their size becomes 0, they disappear but can be restored using the short code (-)
* Using short codes to show/hide the bars. (+)
Each bar has it's own short code.
There is no animation, when showing or hiding.
No possibility to show/hide the bars manually (-)
* Top right corner --> drag and create so a new window.
Each window shows the same part, but it's view is independent. (+)
To delete the window: Drag the corner back (-)
* Items in a bar can be moved manually, but only in the bar itself.
The bar itself is fixed and can't be moved.
* Items in the bar can be collapsed, in order to save space and make it cleaner.
* The tools in the toolbar are stored in tabs.
* When creating a new project, all settings are restored to default, so nothing can be destroyed. (+ probably give the user opportunity to choose)
// TODO: use table instead of + and -?

==== Photoshop / Illustrator (Design tools)
// TODO: (MARCO)
* Photoshop (Image tool, Pixel based)
**

* Illustrator (Design tool, Vector based)
** Multiple windows possible. Are per default in the background opened. Navigation is done by tabs. (+)
** Tabs can be navigated to other places.
This is done by drag and drop
** Tabs can be placed anywhere in the application (- Needs for sure to be discussed. Has certainly it's advantages)
** Workspace can be restored to default using the equivalent setting. (+)
Custom workspaces can be stored. (+)
Multiple workspaces can be stored and it can be switched to. (+)
** Tabs
*** When double-clicking on the Tabs, they collapse (+)
*** They have 3 states (not likely to understand) and it's collapsing behaviour is not intuitive. (-)
**4
// TODO: use table instead of + and -?

==== IntelliJ
// TODO: (MARCO)
* all possibilities available,
Right click on the tab, then one can decide the behaviour of it.
* "Remove from sidebar" removes the feature from the sidebar and it's not intuitive to restore this. (-)
* "Restore default layout" doesn't restore all layout changes (-)
* Layout changes should be stored at one place.
*
// TODO: use table instead of + and -?

==== Adobe XD
// TODO: (MARCO)
// TODO: (mehr fokus auf ist eifach, keine docking elemente usw., was unterscheidet von anderen)

==== To IDE or not to IDE?
// TODO: (MARCO)
// TODO: (nur grob erklären, was erkenntniss, bei IDE: docking framework notwendig, modular approach hineinnehmen)
// TODO:
Power users need the utmost of customizability, to ensure they can adapt the workspace to be the most ergonomic to fit their workflow.
In terms of the architecture, it could mean that it would need to allow the following:

* Separation between `Control` and `Skin`
** Skin can be replaced by API users, if they want to use a custom style
* Flexible panes
** can be rearranged across the whole window
** different modes for fixation
*** Pinned mode
*** Docked mode
*** Floating mode
*** Window mode
*** Split mode

=== User Stories
// TODO: (MARCO)
// TODO: (Workshop with Customer, was very effective, Dieter Zitat, was especially effective because customer mentioned a different persona which changed everything)

==== IDE Decision
// TODO: (MARCO)

==== Differentiation to Eclipse RCP
// TODO: (MARCO)

=== Minimum Viable product (MVP)
// TODO: (MARCO)
As a result of our research we have enough information to create the minimum viable workbench for our use case.
Nonetheless we need to break down the functionality to it's simplest scenario.
// Kunde weiss, dass er ein minimales PRodukt erhält, welches er benutzen kann
// Wir stellen durch MVP sicher, dass ein shared understanding vorhanden ist, und wir vom gleichen reden
This way, we can assure our customer, that the minimal viable product as a result from this project. // TODO: ????
The reason why we do this is, that this way both parties are talking the same language regarding the expectations of the outcome of the project.
Furthermore it's an assurance for both of them.

Characteristic for all programs is: // TODO: move up as conclusion of comparison of programs

* A menu-bar on top of the application.
* Below the menu-bar is often a tool-bar, which contains the current, or most important tools represented through buttons (without collapsing).
* In center is often the window, in which the work is done.
* Usually there are on the left and right of the application bars, which can collapse.
They contain either further tools, buttons or a tree-view for navigation.
* Sometimes another bar which is collapsible is set below the main-window.
* Finally another tool-bar is set below the application.
It contains the least used tools, or tools which are needed at the end of the process.

The outcome of our research results in a minimalistic version of the workbench, our customer developed. // TODO: rewrite
The MVP is designed with the prototyping-tool `Figma`.
It's clickable prototype can be found at:
https://www.figma.com/proto/LY7jPWrDVQ5GG1zmvBdlA2MT/WorkbenchFX?scaling=contain&node-id=47%3A129[figma-prototype]

Below it is shown the final prototype:
[cols="1,1"]
.Minimal Viable Product (MVP)
|===
|link:include/mvp/home.png[image:include/mvp/home.png[Home Screen]]
|link:include/mvp/module.png[image:include/mvp/module.png[Module Screen]]
|===

=== Naming

==== Module Naming
To plug in functionality by the API user, we use modules.
However, the name `Module` is not ideal, since that name already has a lot of associations.
Additionally, when typing `extends Module` in an IDE, it will automatically imports Java's `Module`.
This makes it frustrating, as the implementor has to manually change the import.

To avoid confusion, we want to come up with a name which makes it clearer and less ambiguous.
We did a brainstorming to come up with ideas for possible names for `Module` (<<img-brainstorming-module>>).

.Brainstorming of possible names for `Module`.
[#img-brainstorming-module]
image::include/brainstorming_module.jpg[Module Brainstorming]

We discussed this with our customer and even though we did the brainstorming and came up with a lot of names, none of them seemed to feel "right".
Even though the name `Module` has a lot of associations, it's the only name that makes sense for what it stands for.
In the end, we decided to rename `Module` to `WorkbenchModule`.
We thought it would be easier to understand for our API users and that was the most important factor for us in this decision.
This is why we decided to stick with the word `Module`.
However, we decided to add the `Workbench` prefix, to remedy the importing issue mentioned above and to clearly separate it from the other `Module` classes in the JDK.

NOTE: See the chapter <<_add_module_page>> for the explanation of what a module is.

==== Navigation Drawer
We chose the name "Navigation Drawer" to be consistent with the naming in the https://material.io/guidelines/patterns/navigation-drawer.html[Material Design guidelines].
This makes sure we use the naming that will be the most familiar among other developers and is easily understandable.
An additional benefit is that if someone doesn't know what it means, they can simply look it up in the https://material.io/guidelines/patterns/navigation-drawer.html[Material Design guidelines].

=== API Design
API Users of WorkbenchFX don't only care about the API design, but also about the customizibility.

Our customer states that the intended API user would not have the need to entirely replace the `Skin`, as it is rarely done.
At first we didn't separate between `Control` and `Skin` for this reason.
Later on, our customer told us he would still consider the `Workbench` to be a `Control`, and that it would make more sense if used with https://gluonhq.com/products/scene-builder/[Scene Builder].
This is why we decided to have the `Workbench` extend `Control` and split it up into `Workbench` and `WorkbenchSkin`.
However, since we decided the API user would not want to replace the `WorkbenchSkin`, we decided to make `Workbench` and `WorkbenchSkin` final.
With this, we make it clear that `Workbench` and `WorkbenchSkin` are not designed to be subclassed.

This also means that we don't need to design the panes we use internally to be replaceable by the API user.
It is more important that the user experience is straightforward.
Still, the possibility of extending the workbench using modules is important to our customer.


=== Usability Tests
// TODO: (MARCO)

==== Dieter Holz, Advisor
// TODO: (MARCO)

===== Tabs mit Module Toolbar zusammen => gemeinsame Designsprache automatisch TODO

==== ***REMOVED***, UX Expert
// TODO: (MARCO)

===== Home Icon => HomeView, then AddModuleView => + icon

==== Annelore Egger, User Group
// TODO: (MARCO)

===== Dropdown => ToolbarItem

===== 2 Bars instead of 1 Bar

== Implementation
// TODO: (in the beginning, say that have a look at test modules, and implementation to see all features exactly in details, in action)

=== Layout
// TODO: (MARCO)
Challenge:
// TODO: ????

* Broad spectrum of usage possibilities (-> reduce to one central use case / user story)
* Unique glossary --> Like in the previous project, we aknowledge, that a shared understanding with the customer is needed.

Thus, we're going to define a unique glossary.
So everyone talks the same language.

To gather information and best practices in order to fulfill the needs of the project, we're going to look at several applications.
This includes features, usability and general appearance of the specific workbench.

==== Add Module Page
// TODO: (familiar from smartphones => Modulares konzept, mehrere kleine Applikationen, aber Application macht nicht Sinn als Name, verlinken auf Module Naming)

==== Tabs
// TODO: (from browsers etc., is more familiar)

==== Changes across Versions
// TODO: (visual, with screenshots), tabbar scrollbar why

==== Easy Styling
// TODO: (durch Fabian einfach Stylebar, war fokus, dann beispiel dark theme, von wegen ist so einfach machbar! betonen)

=== Architecture
Since WorkbenchFX will be made open-source as a framework, the architecture is a critical part.
We need to have a good architecture so it will be possible for contributors in the future to extend WorkbenchFX with new features via pull requests.
Also, we need to think about which API's to expose, since these are the API's we need to support in the future.
Once WorkbenchFX is launched to the public, changes to the public API need to ensure they are not breaking changes.
This means we can only very carefully introduce changes later on, and need to make sure the architecture is already good enough, so it doesn't have to be fundamentally changed after the public release.

==== Constructing the Workbench
Since we want to enable the API user to customize the workbench as much as possible, we need to think about in which way the API user should need to interact with our API to do so.
Of course, the resulting API design from this needs to work with our implementation as well.
When we implemented the pagination on the add module page, we wanted the API user to be able to choose the amount of modules shown per page.
To do so, our initial idea was to design the API for creating a `Workbench` object like this:
[source,java]
----
Workbench.of(module1, module2)
           .modulesPerPage(10);
----
This would be very easy to use, but it turned out to be not practical, since the `GridPane` with the module tiles are being initialized in the constructor of `Workbench`.
Changing the amount of modules per page after the constructor was called, would mean that we would have needed to rebuild all of the pages with the modules again.
This is not only very inefficient, but also a very bad solution for this problem.

Another way of solving this would've been to initialize the `Workbench` object with a separate method after setting the amount of modules per page, like this:
[source,java]
----
Workbench.of(module1, module2)
           .modulesPerPage(10)
           .init();
----
This would mean that the `GridPane` with the tiles would only need to be built once.
However, this solution is also not very elegant.
If the API user doesn't want to set the amount of modules per page, they still need to call "init()".
Also, in this case the API user must remember to call `init()`, which is easy to forget.

One of the better options would be to solve it like this:
[source,java]
----
Workbench.of(10, module1, module2);
----
We would simply pass in the amount of modules to the `.of()` method.
This would work, however it has some disadvantages.
For example, the readability suffers: "What does that 10 mean again?".
Also, since we want the API user to be able to define their own controls for the tabs and tiles using factories, we noticed that we also need to pass those factories in the same way.
This would not only make the readability worse, but this also means that if we want to stick to our paradigm, we would need to add multiple overloaded `of()` methods.
With 3 parameters (modules per page, tab and tile factory) this would result in the combinations in <<combinations-overloaded-workbench>>.

.Combinations of overloaded Workbench.of() methods
[#combinations-overloaded-workbench]
|===
|No. |Modules per Page |Tab Factory |Tile Factory

|1
|
|
|

|2
|{y}
|
|

|3
|
|{y}
|

|4
|
|
|{y}

|5
|{y}
|{y}
|

|6
|
|{y}
|{y}

|7
|{y}
|{y}
|{y}
|===

Only 3 parameters result in 7 overloaded `of()` methods, which is already quite a lot.
Should we need to add more parameters in the future, it would get even worse.

This is why we decided to go with our final solution, to *use the builder pattern*.
Using it results in the following syntax:
[source,java]
----
Workbench.builder(module1, module2)
           .modulesPerPage(10)
           .build();
----
This solution solves all of the problems.
It's not possible to forget `build()`, since else it won't return a `Workbench` object.
It's expandable to a large amount of parameters.
It allows for maximum flexibility, i. e. any combination of the parameters in any order can be specified.
We decided against keeping the original `Workbench.of(module1, module2)` notation, since using the builder doesn't require a lot more code and doesn't introduce more complexity.

==== WorkbenchModule Lifecycle
The following UML diagrams explain how the lifecycle of a `WorkbenchModule` works.
The diagrams are simplified to a degree that is relevant for the implementor of a `WorkbenchModule` and are *not* complete and *not* exhaustive by design.
The goal is to explain the principle of interactions on modules when using `Workbench#openModule(WorkbenchModule)` and `Workbench#closeModule(WorkbenchModule)` with a minimal amount of examples.

*Generally*, when clicking on a tile in the `AddModuleView`, `Workbench#openModule(WorkbenchModule)` gets called. +
When clicking on the `x` icon of a tab, `Workbench#closeModule(WorkbenchModule)` gets called.

Overview of the lifecycle in a state diagram, showing all state changes that can occur (<<img-module-lifecycle>>).

.Module Lifecycle.
[#img-module-lifecycle]
image::include/UML/svg/Module Lifecycle.svg[Module Lifecycle]

Process of opening two modules in sequence, followed by opening the first module again (<<img-open-module>>).

.Opening of modules.
[#img-open-module]
image::include/UML/svg/Opening of Modules.svg[Opening of Modules]

Two open modules, closing of the active module (<<img-close-module-active>>).

.Closing of modules, active module.
[#img-close-module-active]
image::include/UML/svg/Close Module Active.svg[Close Module Active]

Two open modules, closing of the inactive module (<<img-close-module-inactive>>).

.Closing of modules, inactive module.
[#img-close-module-inactive]
image::include/UML/svg/Close Module Inactive.svg[Close Module Inactive]

Two open modules, closing of the active module, where the call to `WorkbenchModule#destroy()` returns false (<<img-close-module-interrupt-active>>).
This leads to the closing process getting interrupted.
The implementor of `WorkbenchModule` can then choose to do cleanup actions or open a confirmation dialog, following a call to `WorkbenchModule#close()`, when the module should definitely be closed.

.Closing of modules interrupted, active module.
[#img-close-module-interrupt-active]
image::include/UML/svg/Close Module Interrupted Active.svg[Close Module Interrupted Active]

Two open modules, closing of the inactive module, where the call to `WorkbenchModule#destroy()` returns false (<<img-close-module-interrupt-inactive>>).
This leads to the closing process getting interrupted.
The module which interrupted the closing process will be opened, so that the user's attention will be directed towards the interrupting module, so they can take appropriate actions.

.Closing of modules interrupted, inactive module.
[#img-close-module-interrupt-inactive]
image::include/UML/svg/Close Module Interrupted Inactive.svg[Close Module Interrupted Inactive]

To learn more about the interactions in detail, look at the tests below, since the tests verify the exact order of the calls and cover more situations in detail:

* <<test-reference.adoc#_open_modules,Opening of Modules>>
* <<test-reference.adoc#_close_modules,Closing of Modules>>
* <<test-reference.adoc#_close_modules_interrupted,Closing of Modules Interrupted>>

===== Challenges
Designing the module lifecycle was a challenge we didn't expect.
There were a few goals we wanted to achieve:

* Easy to understand
* Self-explanatory sequence and ordering
* Require the least amount of work from Fabian to use it
* Fulfill the needs of Steffi, cover as many use-cases as possible
* Easy to use, even for complex scenarios

If we make it too complex, Fabian could get confused. +
If we require lots of work to use it, Fabian could get frustrated. +
However, if we make it too easy, Steffi can't cover all of her use cases. +
At the same time, making it more complex potentially makes it less self-explanatory, which in turn makes it harder to use.

The challenge was to strike a balance between complex and easy, ensuring both Fabian and Steffi would be happy with the API.

The first design decision we made was to have `WorkbenchModule` as an abstract class.
This way, we can already pre-define as many lifecycle methods as possible, so Fabian has the least amount of work.
Still, we enable Steffi or even Fabian in some more advanced use cases to override the lifecycle methods they need, to augment or replace the implementation with their own.
The compromise here was to have `#activate()` as the only lifecycle method that *must* be implemented, as it returns the view of the module that should be displayed.
But, since the only thing Fabian has to define is `return view;`, which is acceptable.

We realized every module needs a reference to the `Workbench`, but we can't pass it in the constructor, since at the time of the construction of a `WorkbenchModule`, the `Workbench` doesn't exist yet.
This lead to the `#init(Workbench)` lifecycle method, which initially sets the `workbench` reference, but also allows implementors to initialize their module.

Then, we knew we needed lifecycle methods, so the implementor can know whether their module is the currently active module, which lead us to `#activate()` and `#deactivate()`.
This makes it possible for example to have certain module-specific menu-items in the `NavigationDrawer` or `ToolbarItem`{empty}s in the Toolbar, by adding them with `#activate()` and removing them with `#deactivate()`.

Finally, we need to allow the implementor to free up resources when a module is closed, which lead us to the `#destroy()` lifecycle method.
This was a bit of a challenge itself though, since initially we thought it would be enough to have `void` as the return type.
Then we realized there could be a use case, in which the implementor may want to open a confirmation dialog before a module is being closed.
Again, we thought about different possible solutions and decided to return a `boolean`, allowing the implementor to specify whether the module can be closed or not.
In case of a confirmation dialog before closing the module, the implementor can simply return `false`, causing the closing of the module to get interrupted.

==== API Design of WorkbenchModule
As with the design of the lifecycle, having a simple and easy to understand API overall for `WorkbenchModule` is very important to us.
If there is a lot of code with a lot of methods in the `WorkbenchModule` class, it could make it harder for people to understand how to use the API and how it works.
This is why we very carefully introduced complexity into a `WorkbenchModule`, and only added what really enhanced the user experience to a point that would justify the added code.
We also considered to only add methods which are needed to fulfill a certain use case, which is likely to occur.

===== Added API
One part of a `WorkbenchModule` are its identifying properties.
We decided for a `WorkbenchModule` to have a *name* and an *icon*.
They are necessary, because they are shown in tabs and tiles representing each `WorkbenchModule`.

Every `WorkbenchModule` also has a reference to the `Workbench` object.
This makes it possible to individually adapt the `Workbench` by calling `getWorkbench()`.
This enables the `WorkbenchModule` to have full control over the `Workbench`, for example to switch to other modules.

When we added the module toolbar, we also thought it would make the most sense to include the lists with the `ToolbarItem`{empty}s in the `WorkbenchModule` itself, as they are specific for each module.

===== Rejected API
Generally, we decided to not include anything that wasn't strictly specific to a `WorkbenchModule`.

For example, we discussed whether or not to include a list with `WorkbenchModule` specific `MenuItem`{empty}s to be shown in the `NavigationDrawer`.
In the end, we decided against it, since the `NavigationDrawer` contains global `MenuItem`{empty}s.
If there was one list in `Workbench` and one in each `WorkbenchModule`, API users could get confused.
API users who don't know of the global list in the `Workbench`, could falsely think the `NavigationDrawer` is specific to each module, causing them to add the same `MenuItem`{empty}s in each `WorkbenchModule`.
Additionally, some possible open questions it could cause would be:

* Is there a separate module-specific `NavigationDrawer`?
* Does the list of `MenuItem`{empty}s in the `WorkbenchModule` override or augment the list in the `Workbench`?
* How are the module-specific and global `MenuItem`{empty}s separated?
* Are the module-specific `MenuItem`{empty}s inserted on the top or the bottom?

API users are still able to define module-specific `MenuItem`{empty}s with the already existing API, by adding them to the global list in `activate()` and removing them in `deactivate()`.
This way, we give the API users the choice of how to include them and they don't miss out on any functionality.
It also eliminates the "guessing" of how it was implemented, as it can be observed above in the possible open questions.

==== Icon: Node vs File vs Image
To specify an icon for a `WorkbenchModule`, there are different ways to implement this in the API.

===== Node
The most generic option would be to have the API user pass in a `Node` in the constructor to use as the icon.

The problem is that we display the icon in two places: one in the `Tab` and one in the `Tile`.
The JavaFX API of `Node` is designed to have zero or one parent cite:[noauthor_node_nodate].
This means the API user would need to pass in two `Node` objects, one for the icon of the `Tab`, and one for the `Tile`.

Since the icon will always be the same for the `Tab` and the `Tile`, this feels very redundant and is very error-prone.
Should the implementor not know about this, they could pass in the same `Node` twice and wonder why only one icon is being displayed.
This could cause the implementors to believe this to be a bug.
Sure, we could check in the constructor if both `Node`{empty}s are referring to the same object, but it's still not very user-friendly.

===== File
Another option would be to have the API user pass in a file, referring to the path of the icon.
This way, we don't have the same issues as mentioned in <<_node>>.
However, it is quite limiting, since if the API user wants to use an icon from Font Awesome or Material Design Icons, they would need to find the icon as an image and refer to the file.
This is quite cumbersome, compared to using the `FontAwesomeFX` library and simply specifying `FontAwesomeIcon.GEAR` for example.
Also, this would not allow API users to make use of JavaFX' support for image files with multiple scaling factors, to account for different display resolutions cite:[lemmermann_javafx_2017].

===== Image
We decided to use `Image` as one of the options of passing in an icon in the constructor of `WorkbenchModule`.
There are none of the issues mentioned in <<_node>>, since the `Image` will be wrapped in an `ImageView`, when `WorkbenchModule#getIcon()` gets called by the `Tab` and `Tile`.
Also, it allows API users to make use of JavaFX' support for image files with multiple scaling factors cite:[lemmermann_javafx_2017].

Unfortunately, it doesn't solve the issues with icons from libraries, as mentioned in <<_image>>.
To solve this, we looked at what were the most widely used icon libraries.
Font Awesome is the most popular icon set, which is why we chose to include it as another option in the constructor cite:[noauthor_font_nodate].

Also, since the design of WorkbenchFX is highly influenced by Material Design, we wanted to include an icon set that would include Material Design icons.
FontAwesomeFX offers support for the https://material.io/tools/icons/?style=baseline[official Material Design Icons by Google] and also https://materialdesignicons.com/[Material Design Icons] cite:[noauthor_fontawesomefx_nodate].
Since all of the icons from the https://material.io/tools/icons/?style=baseline[official Material Design Icons by Google] are included, along with many others in the icon set of https://materialdesignicons.com/[Material Design Icons], we also included a constructor option for https://materialdesignicons.com/[Material Design Icons].

==== Model View Presenter (MVP)
MVP was used as general architecture pattern.
We thought it was overkill to use the `Control` and `Skin` architecture from JavaFX for all internal views as well, especially since they don't have to be replaceable (see <<_api_design>>).
But at the same time, we wanted the views to be separated to be more readable and maintainable.
We chose the MVP pattern because we used it in the https://github.com/dlemmermann/PreferencesFX[PreferencesFX] project and it proved to be very good for those purposes.
This is why we still chose to use the MVP pattern for all internal views.
At first, we used a separate `WorkbenchModel` object for the model.
Our customer suggests that it would be easier to skip the `WorkbenchModel` class and just put the logic in `Workbench`, since that's the place where he would expect such logic.
We decided to change it in a way that `Workbench` is the model object in the MVP pattern in our case.
This simplifies the architecture and readability is improved, since methods are where they would be expected.

`View` is an interface, because it's used as a mixin.
We implemented it this way because every view needs to already extend a certain JavaFX class and multiple inheritance is not possible in Java, so using an abstract class here isn't an option.
This makes it possible to initialize all views in the same way while also resulting in less code duplication, since the `init()` method and the JavaDoc doesn't need to be rewritten over and over again.
`Presenter` is abstract, because it acts more like a "super class" of all the presenters.
Since all presenters don't need to extend anything else, using an abstract class here is possible.

==== Overlays
To enable the API user to easily show a custom overlay with the option of having a black transparent `GlassPane` in the background, they can use the respective `Workbench#showOverlay()` and `Workbench#hideOverlay()` methods.

The GUI of WorkbenchFX is organized in layers.
At all times, there is a layer of the general WorkbenchFX GUI, which is being represented by the view class `WorkbenchView`.

When showing an overlay, a `GlassPane` is added to the scene graph and shown on top of the WorkbenchFX GUI, followed by the overlay itself on top of the `GlassPane`.
This ensures there is a "scrim"cite:[noauthor_elevation_nodate] between the WorkbenchFX GUI and the overlay (<<overlay-scrim>>).

.Overlay with scrimmed background
[#overlay-scrim]
image::include/overlay.png[Overlay with scrimmed background]

When hiding an overlay, it is made invisible and is not removed from the scene graph, so recurring overlays don't need to be re-inserted into the scene graph again.
This is more efficient and especially overlays with animations profit from this design, since they run smoother when shown and hidden multiple times.
In case of an application with very memory-intensive overlays, there is a possibility to call `Workbench#clearOverlays()`, which will remove all overlays from the scene graph and free them up to be garbage collected.

Each overlay has its own `GlassPane` and it is possible to open multiple overlays on top of each other.

NOTE: While it is not recommended to open an overlay on top of another overlay, it frees the API user from having to account for timing / concurrency issues (see <<_challenges_2>>) during transitions between overlays.

===== Challenges
At first, we designed the API to not allow multiple overlays to be shown on top of each other.
This enabled us to have one `GlassPane` and one `DialogControl` to use for all overlays.
We would then simply exchange the `WorkbenchDialog` model object in `DialogControl`, which would cause the `DialogControl` to change accordingly.
This prevented the API user from the bad practice of showing multiple overlays on top of each other. +
However, this lead to timing / concurrency issues when switching from one overlay to another.

When closing the stage with two open modules, which both would interrupt the closing process with a dialog, confirming the close on the first dialog would cause no dialog to be shown at all, even though the closing dialog of the second module should've been shown.
It turned out calling `showOverlay()` to show the second dialog was called fractions of a second earlier than `hideOverlay()`.
This resulted in the second dialog to be shown first, which was then hidden with the call to `hideOverlay()`.
Since the whole process is asynchronous and highly depends on the changes in JavaFX's scene graph that are out of our control, we decided to allow multiple overlays to be shown on top of each other.
Since the overlap during the transition between two overlays is so minimal that it can't be seen, this is not a problem visually.
Even if we would've gotten the synchronization right internally, it would still mean an API user choosing to show two subsequent overlays would need to account for timing / concurrency issues as well, which was not acceptable for us.

Initially we also designed the API in a way that would require overlays to be loaded, either with a separate method call or during the creation of `Workbench`, before they could be shown.
While this resulted in the best performance possible, since the overlays could be loaded hidden with the start of the application, it made the API more complex.
Since changes in the scene graph can be performed quite quickly by JavaFX, resulting in the performance benefits to not be significant, we decided to change the API to load the overlays into the scene graph as they are being shown.
But we decided to go for a compromise: we would still not remove the overlays when they are being hidden, to make sure they can be loaded faster the next time they are being shown, since they already have been added to the scene graph.

== Processes

=== Development

==== GitHub
The programming is being done in a GitHub repository.
We work using the git flow branching model cite:[noauthor_successful_nodate].
Every change is represented in a pull request to develop from the feature branches.

==== Code Review
To improve the quality of the code and also ensure https://www.agilealliance.org/glossary/collective-ownership/[collective code ownership], every pull request gets code reviewed by the other person.
This worked very well for us, since comments that were made by the other person always lead to improvements and code of higher quality.

==== SCSS instead of CSS
// TODO: (MARCO)
// TODO: Explain reason why we use SCSS instead of CSS
Less code.
Easier to read.
Plugin for IntelliJ which translates SCSS code into CSS:
https://www.jetbrains.com/help/idea/transpiling-sass-less-and-scss-to-css.html[SCSS to CSS]

There is no margin in JavaFX CSS.
We've made a workaround using a `SCSS Mixin` which looks like this:

Creating the `Mixin`:
[source,sass]
----
@mixin margin-all($margin) {
  -fx-padding: $margin;
  -fx-border-insets: $margin;
  -fx-background-insets: $margin;
}
----

Using the `Mixin` in code:
[source,sass]
----
.my-class {
  @include margin-all(1.5em);
}
----

A `Mixin` includes code, created by the `Mixin` and puts it in the place where it was called.
Link: http://thesassway.com/advanced/pure-sass-functions[SCSS - Mixin]

==== General Conventions
// TODO: (MARCO)
// TODO: (CSS, PreferencesFX kopieren GOogle Java style guide)
We have decided to define some general conventions:

In CSS it is possible to give multiple classes the same layout.
For example:
[source,sass]
----
.myClass-1, .myClass-2 {
  -fx-padding: 1em;
  -fx-border-insets: 1em;
  -fx-background-insets: 1em;
}
----
this is possible, but it's readability is not that good.
SCSS provides a special tag for such situations, the `@extend`-tag:
[source,sass]
----
.myClass-1 {
  -fx-padding: 1em;
  -fx-border-insets: 1em;
  -fx-background-insets: 1em;
}

.myClass-2 {
  @extend .myClass-1;
}
----
In both cases, both classes have the same attributes.
But the readability is much better in the second example.
Plus, there is the possibility to add another attributes in `.myClass-2`.
[source,sass]
----
.myClass-2 {
  @extend .myClass-1;
  -fx-color: RED;
}
----
`.myClass-2` inherits from `.myClass-1` and can be extended.
https://sass-lang.com/guide[SCSS - @extend]

Another convention we use is the separation of all scss-files.
For readability we have the `main.scss` file separated into multiple scss files.
In the `main.scss` we include all those other files and this "main file" will be compiled as a css file.
Using the `@include` tag looks like this:
[source,sass]
----
@include file1;
@include file2;
@include file3;
----
The files to be included are named with an underscore as prefix: `_file1.scss`.

=== Testing
Code quality is one of our main priorities in this project.
Since the framework is meant to be used in enterprise applications as well, WorkbenchFX needs to be robust.
Also, we wanted to rather have less features, but of better quality.
One of the criteria in the definition of done is having unit tests for the code.
We would not merge a pull request before the tests are all done and passed.
This improved the code quality by a lot, since we found around two bugs (sometimes less, sometimes more) per pull request during testing.

We thought about using Test Driven Development (TDD), but because the framework is new, the API was not clear from the beginning.
Since we had a focus on good "usability" of the API, it changed a lot during development.
This is why we decided not to do TDD, because it meant we would have needed to constantly adapt the tests to the changing API in the beginning of the development, which would be very inefficient.

Unit testing also had the advantage that it "forced" us to have a good architecture.
If the architecture isn't good, it is very hard to test in an isolated way.
We employed the pattern of _Dependency Injection_ whenever we could, since that made one of the biggest differences in testable code for us.

==== JUnit vs Spock
Most of our unit testing, especially in the beginning, was done by using JUnit.
However, when there were some test cases that were data-driven.
JUnit 5 does have data-driven testing capabilities, but it doesn't offer much flexibility and the way tests have to be implemented to be data-driven is not very readable cite:[bechtold_junit_nodate].
An example can be seen in <<junit-parameterized>>.

.Parameterized Tests with JUnit 5 cite:[bechtold_junit_nodate]
[source,java]
[#junit-parameterized]
----
@ParameterizedTest
@ValueSource(strings = { "racecar", "radar", "able was I ere I saw elba" })
void palindromes(String candidate) {
    assertTrue(isPalindrome(candidate));
}
----

Also, JUnit 5's data-driven testing API is still considered "experimental" cite:[bechtold_junit_nodate].
We didn't want to risk all of our data-driven tests becoming obsolete, should JUnit later choose to change the concept entirely.
Combined with the readability argument, we decided not to do data-driven testing with JUnit.

At first, we proceeded to write the data-driven tests as usual in JUnit.
But they had lots of duplicated code and therefore were not very maintainable (see <<comparison-junit-spock>>).
We decided to invest some time to find a better solution.

We already knew Spock as a testing framework, but we have never used it before in a Java context, since Spock is written in Groovy.
However, Spock has one big advantage over JUnit: Data driven testing is very well implemented.
Parameters in data-driven tests are implemented in a table-like format (see <<comparison-junit-spock>>).
A test case can be built fully parameterized, which means the tested method and assertions only need to be written once.
Spock will then dynamically run the test case multiple times with each row of the table-like data structure as parameters cite:[niederwieser_data_nodate].
This makes the code much more readable and maintainable.
It also means adding another data set after the fact just means adding another row to the data, not copying and modifying unit test code over and over again.

However, it was still unclear whether we could use Spock to test Java code and even more so JavaFX applications, which are already more difficult to test.
In theory it should be possible, as Spock claims on their website cite:[noauthor_spock_nodate].
At least TestFX (see <<_testing_utilities>>) offers support for Spock, so it seemed like a viable option.
We did some research on GitHub, but we didn't find any Java projects which used *JUnit 5* together with Spock and *Maven*.
After a lot of challenges, we were able to make it work.
The key part to the solution, was that we needed to add the JUnit Vintage Engine, which is the engine that enables to run JUnit 3 and 4 tests cite:[bechtold_junit_nodate].
Everything that was needed to make it work can be seen in the <<misc-reference.adoc#_maven_code_pom_xml_code_workbenchfx_core,`pom.xml` file of `workbenchfx-core`>>

However, the end result was well worth the effort.
We rewrote the data-driven JUnit 5 test with lots of duplicated code mentioned above with Spock and compared both in <<comparison-junit-spock>>.

.Comparison of a Data Driven Unit Test with JUnit 5 and Spock
[#comparison-junit-spock]
image::include/comparison_junit_spock/comparison_junit_spock.png[Comparison of a Data Driven Unit Test with JUnit 5 and Spock]

Not only is the unit test written with Spock much more readable, but also easier to maintain.
The output is much easier to read with Spock as well, since it creates a separate test for every row in the parameters.

==== Testing Utilities
There were a few testing utilities we used to augment the already existing features of JUnit and Spock.

===== Mockito
We used Mockito for mock testing with JUnit.
Used in combination with the dependency injection pattern, it enabled us to test classes in a more isolated way.
For example, it allowed us to mock the view classes, so we could test presenter logic separately.

===== TestFX
Using TestFX made it even possible to test JavaFX code.
Without it, JavaFX will throw `IllegalStateException: Toolkit not initialized` for all method calls that run on the GUI thread.
TestFX also offers the possibility of automated GUI testing, which we decided not to make use of because of the fragile nature of those tests.
We used unit tests whenever possible and when it wasn't, we used integration tests.
See the tests in `workbenchfx-core/src/test` for more information.

===== Awaitility
After we implemented the animations for the drawers, some tests were failing.
Because of the animations, there is a delay of 200 ms before the drawer gets hidden.
The problem is, the assertions which check if the drawer is not visible run before the animation has finished (the drawer is hidden).

This was quite a challenge, since `Thread.sleep()` and other strategies didn't help at all, as they probably were just sleeping the GUI thread, which didn't help with the issue.
We then stumbled upon _Awaitility_, which was made for testing asynchronous logic.
By using it, we were able to specify a condition and a time frame, in which the condition(s) should be met (see <<awaitility-unit-test>>)
Awaitility does the rest and performs the assertion.

We found Awaitility to be quite useful for this purpose, as it worked flawlessly and was easy to use.

.Unit test with Awaitility
[source,java]
[#awaitility-unit-test]
----
include::{dir-test-java}/WorkbenchTest.java[tags=awaitility]
----

=== Building

==== Travis CI
To simplify the code review process, we are using Travis CI.
Travis CI is a build server similar to Jenkins.
Compared with Jenkins, it runs in the cloud and is much more straight forward to set up in our experience.
The biggest advantage is the tight integration with GitHub, which for example allows automatic building of pull requests.
GitHub then directly shows the build status in every pull request and we also set up that a pull request can't get merged until the build passes.
With every build, Travis will compile the code, run checkstyle and tests.

We agreed with our customer to use the Google Java Code Style guidelines, as they are used in one of the most-widely used libraries in Java, Guava cite:[idan_top_2017], and because of its well-maintained open source checkstyle configuration on GitHub cite:[noauthor_checkstyle:_2018].
If checkstyle finds code style violations, we set it up to fail the build.
This provides us with immediate feedback when we open a new pull request and forgot to run checkstyle checks ourselves.
Also, it makes it easier for the reviewer, since they don't need to run the tests and checkstyle themselves every time.

In order to get build results from Travis faster we enabled "Auto Cancellation".
If there are a lot of pushes from the same branch or pull request, Travis will then cancel all but the most recent build in the queue for each branch or pull request.

Travis makes two checks each time you make a push to the repository.
The first check is the "push-check", which tests the compatibility of the current branch (<<travis-check>>) cite:[noauthor_travis_nodate].
The second one is the "pr-check", which emulates a merge with the target branch in order to check if the merge leads to errors making the build fail (<<travis-check>>) cite:[noauthor_travis_nodate].
This has the huge advantage that our work is not only easier but also safer to accomplish.

.Check by Travis CI on a GitHub pull request
[#travis-check]
image::include/travis-check.png[Check by Travis CI on a GitHub pull request]

==== Codecov.io
Since testing was one of our main tools to ensure good code quality, we wanted to also make use of code coverage.
This is where codecov.io came in.
It's a platform that visualizes code coverage and also integrates nicely with GitHub and Travis.
The code coverage itself is measured by https://www.eclemma.org/jacoco/[JaCoCo] and is specified in the `pom.xml` of `workbenchfx-core`.
We set it up so that every successful Travis build would push the code coverage to codecov.io.
Thanks to the GitHub integration, a codecov bot would create a comment on every pull request, that gets updated with pushes to the branch of the pull request (<<codecov-comment>>).
Also, codecov.io would show a "check" in GitHub and would only pass if certain conditions are met.
One of those is that the code coverage must not be lower than it previously was (<<codecov-check>>).
The other one was the `diff` coverage, that measured how much of the added code was covered (<<codecov-check>>).

Codecov.io also enables to set exceptions on certain classes, which should not be taken into account for the code coverage.
We mainly did this for view classes, since they cannot really be unit-tested.
Also, we added abstract classes and interfaces to the exceptions, since it wasn't possible for JaCoCo to recognize executions of lines in them.
See <<misc-reference.adoc#_codecov_io_exceptions_codecov_yml,Codecov.io Exceptions>> for all of the concrete classes we removed from code coverage.

We didn't set a goal for the code coverage per se, since we didn't want it to lead us to try to test everything.
But in general, we tried to keep the code coverage over 90%, which is already quite high.
In the end, we were able to reach a code coverage of *94.67%*.

We realized code coverage is a useful tool, but you can't solely rely on it.
For example, if we added a lot of new fields with mutators and accessors, we noticed we almost always weren't able to meet the goals defined by codecov.io in the GitHub checks.
This is because getters and setters are not tested cite:[osherove_art_2013, page=11].
This leads to the coverage results getting skewed, since there is more code that is also not covered, reducing the code coverage.
In this case, it is the creator of the pull request's responsibility to check, whether there is untested code that needs to be covered or if it is just the mentioned effect.
Also, it is the reviewer's responsibility to validate the creator's decision in this regard, as sometimes checking the code coverage can be forgotten.
If the results were skewed, we went with the pragmatic approach and chose to still merge the pull request.
This approach worked very well for us and our decision payed off, given the high code coverage we reached.

Still, code coverage can be a double-edged sword.
As we saw, a lower code coverage doesn't always equal less code quality.
As we realized, this is also true for the opposite.
Simply because a line of code was executed during a test, doesn't necessarily mean it was tested properly.
That's where code review comes in, and we had to remind ourselves constantly to not rely on code coverage *too* much.
We still need to check whether all of the edge cases have been tested and if the tests really perform all necessary verifications.

However, using codecov.io proved to be really useful to us.
We had a quick overview over the code coverage and as with Travis, it was one step less in our code review process.
We didn't always have to build the branch to see the code coverage and the checks on GitHub doubled as a reminder to have a look at the code coverage.

.Example comment on GitHub by the Codecov.io bot
[#codecov-comment]
image::include/codecov-comment.png[Example comment on GitHub by the Codecov.io bot]

.Check by codecov.io on a GitHub pull request
[#codecov-check]
image::include/codecov-check.png[Check by codecov.io on a GitHub pull request]

=== Releasing
When working agile, releasing often is important cite:[raymond_cathedral_2001, page=28].
Since we are only in a team of two, we think it wouldn't make sense to have a strict release plan.
Instead, we opted to release whenever it makes sense.
This was mainly for example when a new major feature was introduced and there are no other major features expected in the next few days.

Concerning the version numbering scheme, we chose to adhere to the following: vX.Y.Z

* X
** 0 -> before we reached the MVP
** 1 -> after we reached the MVP
** 2 -> final version to hand in for the bachelor thesis
* Y
** Starting from 0, incremented by 1 with every major release
* Z
** Starting from 0, incremented by 1 with every hotfix release

==== Process
Over time, we established the following process for releases:

. Checkout `develop`, pull and start a release with git flow
.. Example branch name `release/v0.7.1` for version v0.7.1
. Create a pull request (PR) from `release` branch to `master`
.. Title: version number (for example "_Release v0.7.1_")
.. Description: enter what has changed since the last release
. In the `pom.xml` file of `workbenchfx-parent`:
.. Update the property `workbenchfx.version` with the new version number (for example `0.7.1`)
. Run `mvn install -DskipTests` to automatically propagate the new version number to all files
. Commit and push to the release branch
. Have the release PR approved by a member different from the one creating the PR
. Merge the pull request
. Checkout `master`
. Run `git tag v0.7.1`, followed by `git push origin v0.7.1` (each with the new version number)
. Go to releases on Github and edit the release when Travis has finished the deployment
.. Edit title and description to match the one of the release PR
. Checkout develop
. Merge master into develop to ensure the version changes are also present there

==== Release Automation
We also automated part of the release process with Travis.
When we push a git tag on the master branch, Travis will run `mvn install`, make zips of the documentation and javadoc, followed by creating a release on GitHub and uploading the zips along with the built `jar`{empty}s.
The only thing left is to edit the title and description with the release and its changelog, respectively (see <<_process>>).

TIP: This made releasing quicker and meant less repetitive effort on our side.

Details on the implementation can be found in the `.travis.yml` file.

== Lessons learned
// TODO: (we did already in the start was good idea since lots of things improved, erwähnen PreferencesFX Lessons Learned => refactoring, testing, usw, checkstyle, javadoc gerade gemacht)

=== Value of User Stories
// TODO: (MARCO)
// TODO: (How good was workshop etc.)

=== Working Agile
Even though we had to face a lot of challenges, there is one thing we are especially grateful about: working in an agile way. +
Because of the agile approach and having the whole development process transparently on GitHub, we were able to get constant feedback from our customer.
This way, issues or misunderstandings in communication quickly became obvious and we were able to adjust the implementation accordingly right away.
This enabled us to develop a product of very high quality, which meets the demands of our customer and future users alike. +
So in the end, working agile enabled us to quickly identify and resolve challenges.

For example, the separation between `Workbench` and `WorkbenchSkin` only became obvious later in the project (see <<_api_design>>).
If we didn't get constant feedback from our customer, we would probably only have recognized this when it was too late - at the end of the project.

TIP: Thanks to the agile way, we could resolve challenges quickly and early on in the project.

=== Hacking Day
// TODO: (MARCO)
// TODO: (Dirk hat selber erkannt, dass nicht so einfach wie man es sich vorstellt, dass es Dirk auch etwas gebracht hat zu erkennen => besser im Projekt eingebunden, besser gemerkt wo Probleme sind)

=== Animations
One thing we underestimated by far was the power of animations.
Initially, we didn't have animations on the drawers.
// TODO: maybe add link / reference to above somewhere?
In a usability test with Dieter Holz we noticed that especially with the drawers that covered most of the screen, it was not clear where they were coming from or that they even were drawers.

When we implemented the drawer animations, we were shocked to see how much of a difference it made.
We expected it to make a slight difference, but it really improved the user experience by a lot.
Since it makes that much of a difference, we think that even if there was an option to turn off animations in the future, it should not be possible to turn off drawer animations.
The suggested movement of the drawers is a key element to the user experience and must not be removed.

TIP: Even though the implementation of the animations was quite complicated, the end result was well worth the effort.

=== Switch from Gradle to Maven
Initially, we used Gradle, since we already used it in https://github.com/dlemmermann/PreferencesFX[PreferencesFX] and it was more convenient to re-use the already existing `build.gradle`.
However, when it came to publishing PreferencesFX to Maven Central, we noticed it was significantly more complex to set it up with Gradle than with Maven cite:[noauthor_contribute_2018].
Since our customer mainly uses Maven for his projects and already knows how to set it up for publishing to Maven Central, we decided to switch to Maven for PreferencesFX.
Since we did it for PreferencesFX and we knew the day would come when we would face the same issue with WorkbenchFX as well.
This is why we switched WorkbenchFX to using Maven as well.

We didn't regret the change at all, especially since our team and our customer had struggled quite a lot with resolving dependencies with Gradle.
We would run into issues which required us to re-import the project every now and then, which was quite cumbersome with time.
Sure, we also faced our issues with Maven as well, but we feel like it is much less than it used to be with Gradle.

TIP: This is why in the next project we would start with Maven right away.

=== Build Automation
The automated build we set up with Travis CI was very useful, in fact it would have been even more useful if had set it up from the beginning.

When we released `v1.1.0`, while building the binaries for the release on GitHub, we noticed there was a checkstyle error which prevented the build from completing.
This was embarrassing.
The release already happened and the pull request was merged already, so there was no way back.
We had to release a hotfix `v1.1.1` with the checkstyle fix in place, to be able to build the binaries.

Even though we had a process in place (see <<_process>>), that should've prevented this situation, it happened.
This proved again that having processes in place is good, but people make mistakes.
It's easy to forget a step in a process or to forget running `mvn package` during code review.

This is where automation comes in.
It removes steps in repetitive processes and is especially useful when the steps are prone to error.
Since the code review and releasing process is mostly the same, there are a lot of steps to forget and errors to make, as reviewing the code itself is already demanding enough.
If we had already used Travis CI from the beginning, this mistake wouldn't have happened.
Travis' checks on the release pull request would've failed and we would've instantly noticed that something is wrong.
This would've enabled us to correct the error and then start the release process from anew.

Automation doesn't only prevent errors, but also leads to substantial time savings in the long run.
In the end, we had performed 85 pull requests and 11 releases, resulting in 1160 builds on Travis CI.
We can only imagine how much time we saved.
Setting up Travis CI also took time, but we are sure it already saved us more time than the setup required.

TIP: We only started using Travis CI with `v1.3.0`, in the next project we would use it from the beginning.

=== Java 8 / 9 / 10
In the beginning of the project, Java 10 was just released.
Our customer wanted us to use Java 10, because he noticed some JavaFX-related bugs in Java 9 that were fixed in Java 10.

A bit later on in the project, he told us to switch back to Java 9, as he noticed dependencies were not ready for Java 10 yet, since most of the other developers have not switched to Java 10 yet.
We used Java 9 for the longest time, but we started to worry because Oracle started to deprecate Java 9 cite:[noauthor_java_nodate] and it became more difficult to download, even requiring an Oracle login to do so.

One of the user stories mentioned the ability to use WorkbenchFX with https://www.jpro.one/[jpro].
To see if it works, we did a proof of concept with jpro and noticed that in fact it did not work.
We thought it could be an issue with jpro not being compatible with Java 9 and decided to try it with Java 8.
Luckily, we haven't used many Java 9 features in the code, so it was mainly switching from Java 9 versions of dependencies to Java 8 versions of them.
And it worked!

This is why we decided to go with Java 8.
It resolves the issues of Java 9 being difficult to download and at the same time, it allows us to make use of a larger user group, as lots of developers are still using Java 8.
We did however decide to leave comments in the `pom.xml` files with the Java 9 versions of the dependencies, so it is possible to publish two separate versions when WorkbenchFX goes open source.
This is something quite a lot of JavaFX libraries and frameworks seem to do cite:[noauthor_controlsfx_2013]+[noauthor_scene_nodate]+[jfoenix_jfoenix_2018] , so we thought it would make the most sense.

However, we don't think this is a good trend.
It seems to us that instead of being open and trying to stay updated with the newer Java versions, most developers do the opposite and try to stay on older versions for as long as they can.
This in turn forces developers of frameworks and libraries to also support the older versions of Java, which makes the problem worse.

[TIP]
====
We're not entirely sure how we would decide in our next project.
Since Java 11 will be released on the 25^th^ of August 2018 cite:[noauthor_jdk_nodate], which is very soon.
Only time will tell how developers react to it, which could totally change any recommendations from our side, especially since Java 8 is starting to get deprecated, with Java 11 being a long term support release cite:[noauthor_oracle_2018].
====

=== Don't underestimate the seemingly most simple tasks
// TODO: (FRANÇOIS)

// TODO: (Closing of modules, how hard it was, completablefuture...)

== Summary
// TODO: (MARCO)

=== Future Implications
// TODO: (what features can be done later?)

== Bibliography
bibliography::[]

== Honesty Declaration
It is hereby declared that the contents of this report, unless otherwise stated, have been authored by François Martin and Marco Sanfratello. All external sources have been named and quoted material has been attributed appropriately.

The signatures are delivered separately.

// TODO: insert place and date here
